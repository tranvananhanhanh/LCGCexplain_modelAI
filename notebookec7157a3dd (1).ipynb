{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7161237,"sourceType":"datasetVersion","datasetId":3884190},{"sourceId":8548287,"sourceType":"datasetVersion","datasetId":5107689}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# import pandas as pd\nimport numpy as np\nimport pandas as pd\n# Đường dẫn đến tệp dữ liệu trên Kaggle\nfile_path = '/kaggle/input/vindr-mammo-dataset/breast-level_annotations.csv'\npairs = []\n\n\n# Load 100 mẫu đầu tiên từ tệp CSV vào một DataFrame\ndf = pd.read_csv(file_path).head(5000)\n\ndf.loc[df['breast_birads'] == 'BI-RADS 1', 'breast_birads'] = '1'\ndf.loc[df['breast_birads'] == 'BI-RADS 2', 'breast_birads'] = '2'\ndf.loc[df['breast_birads'] == 'BI-RADS 3', 'breast_birads'] = '3'\ndf.loc[df['breast_birads'] == 'BI-RADS 4', 'breast_birads'] = '4'\ndf.loc[df['breast_birads'] == 'BI-RADS 5', 'breast_birads'] = '5'\n\ndf_new = df[['image_id', 'breast_birads']].copy()\n\ngroups = df_new.groupby('breast_birads')\n\n\n# Lấy mỗi nhóm một phần tử và kết hợp chúng lại với nhau\nresult = pd.concat([group.head(10).reset_index(drop=True) for name, group in groups])\n\nnew_df = pd.DataFrame(columns=result.columns)\nselected_rows_list = []\n\nfor i in range(0, 10):\n    selected_rows = result.loc[result.index == i]\n    selected_rows_list.append(selected_rows)\n\n    # Hiển thị kết quả\nnew_df = pd.concat(selected_rows_list)\n\n# Print the new DataFrame\nprint(new_df)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-29T13:27:03.479791Z","iopub.execute_input":"2024-05-29T13:27:03.480203Z","iopub.status.idle":"2024-05-29T13:27:03.583682Z","shell.execute_reply.started":"2024-05-29T13:27:03.480173Z","shell.execute_reply":"2024-05-29T13:27:03.582533Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"                           image_id breast_birads\n0  dd9ce3288c0773e006a294188aadba8e             1\n0  d8125545210c08e1b1793a5af6458ee2             2\n0  aad38467d1f04cedf300c9c8305a3536             3\n0  2b46a6fc2c076cfc187b385f6a6f40e6             4\n0  581b0e4d471a5c0adc6888cb038fa722             5\n1  57fbdd278af5c8789a02b355c11620d4             1\n1  290c658f4e75a3f83ec78a847414297c             2\n1  2ccefd9e7701acf52c92275e844f7578             3\n1  8338207743a394ab8b56b5b920fc30b6             4\n1  29896919b15b2dd978a36b2f68a5ad93             5\n2  202d761a6b0f86faaeefc39ee18b1c53             1\n2  cd0fc7bc53ac632a11643ac4cc91002a             2\n2  39a9809abe1441f60f9fab50679c2a73             3\n2  396bea28fd4be11598bc53bf5f2b7460             4\n2  5d13674e2ed04208bf232e2eeb468f7a             5\n3  acccc1727b61b261844d86aa9de53536             1\n3  71638b1e853799f227492bfb08a01491             2\n3  6770e08049264165b58c5f5ddcfd5b95             3\n3  df4b1cfb384dab76abc34cc4db0f5b30             4\n3  d4cae2c614676925578ca87836ab95b6             5\n4  a3d0e2394d7db36afab1b6e5e24da798             1\n4  421afeb5f9dd7b64b84ac8335b14b247             2\n4  35293500d91fb681fd8ff9171d8161c1             3\n4  33755feadab4a6b5db51edc060585921             4\n4  39f3ef0e8046a9b4e6c81262d634053d             5\n5  48b243704d16570155df12995a284b61             1\n5  fb90604f039d294a931b94442e7f8b73             2\n5  2f11b93c073630fc866ebd61708c49c5             3\n5  44d35d157f5796ebc04b5fb51c3c1f12             4\n5  cb02caed6492a9619e93a3bd265a9c7f             5\n6  bd6af21b792a7efb367407e94678fa1f             1\n6  5c40def8eb37a29833a498f9390f2b02             2\n6  305835a194605ef353a1700def9a2429             3\n6  03234e390d3f2c9c07889e1687eafbb8             4\n6  e61d36901ebb6b37ac41f6ca84e5c81b             5\n7  633ede597d7e514fe672fbdfc8c269eb             1\n7  79929caccef83eaa6f23972d59982913             2\n7  d5701f000de83388af9171cea1427a7e             3\n7  c171fed7a869e9cb41c9a10e7ee6f4f9             4\n7  b74f95f32636aff0b94540891c6b1f14             5\n8  1b66d3ea1dae116b7c0e87e3caab3340             1\n8  8132013f5158a2bc93d77ea9200c75d8             2\n8  4285bb1fd50ed9f19bd507e66738189c             3\n8  2530e73814b6a250140fab195ac789cc             4\n8  271e8bfd46adee65f8580092753d08e5             5\n9  7a3df96890c90370590984ca196d1b40             1\n9  09c140f91b2da7c7ff024e5f4b2d5599             2\n9  5e35ead87bb708f1fea95033ddb2e093             3\n9  73d20cbf87ec2637a3de9d950ad809b2             4\n9  3c315903764b853c05c059346675e1b7             5\n","output_type":"stream"}]},{"cell_type":"code","source":"import cv2\nimport pandas as pd\nimport os\nimport torch\nfrom torchvision import transforms","metadata":{"execution":{"iopub.status.busy":"2024-05-29T13:20:50.274826Z","iopub.execute_input":"2024-05-29T13:20:50.275276Z","iopub.status.idle":"2024-05-29T13:20:57.542730Z","shell.execute_reply.started":"2024-05-29T13:20:50.275244Z","shell.execute_reply":"2024-05-29T13:20:57.541427Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nfrom torchvision import transforms\n\ndef load_image(image_path):\n    # Read the image from the path\n    image = cv2.imread(image_path)\n    \n    # Convert the image from OpenCV's BGR format to RGB\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    \n    # Convert the NumPy array to a PIL image\n    pil_image = Image.fromarray(image)\n    \n    # Define the transformation pipeline\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),  # Resize the image\n        transforms.ToTensor()  # Transform the image to a tensor\n    ])\n    \n    # Apply the transformations to the image\n    image_tensor = transform(pil_image)\n    \n    return image_tensor\n\n\n\n# Thư mục chứa ảnh\nimage_folder = \"/kaggle/input/vindr-mammo-dataset/Processed_Images\"\n\n# Đọc DataFrame từ file CSV\ndf = new_df\nprint(df)\n\n# Tạo một cột mới trong DataFrame để lưu tensor của ảnh\ndf['image_tensor'] = None\n\n# Hàm đệ quy để tìm ảnh trong tất cả các thư mục con\ndef find_images(image_id, folder):\n    for root, dirs, files in os.walk(folder):\n        for file in files:\n            if file == image_id+'.png':\n                image_paths=os.path.join(root, file)\n    return image_paths\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-29T13:28:59.760801Z","iopub.execute_input":"2024-05-29T13:28:59.761201Z","iopub.status.idle":"2024-05-29T13:28:59.779663Z","shell.execute_reply.started":"2024-05-29T13:28:59.761166Z","shell.execute_reply":"2024-05-29T13:28:59.778510Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"                           image_id breast_birads image_tensor\n0  dd9ce3288c0773e006a294188aadba8e             1         None\n0  d8125545210c08e1b1793a5af6458ee2             2         None\n0  aad38467d1f04cedf300c9c8305a3536             3         None\n0  2b46a6fc2c076cfc187b385f6a6f40e6             4         None\n0  581b0e4d471a5c0adc6888cb038fa722             5         None\n1  57fbdd278af5c8789a02b355c11620d4             1         None\n1  290c658f4e75a3f83ec78a847414297c             2         None\n1  2ccefd9e7701acf52c92275e844f7578             3         None\n1  8338207743a394ab8b56b5b920fc30b6             4         None\n1  29896919b15b2dd978a36b2f68a5ad93             5         None\n2  202d761a6b0f86faaeefc39ee18b1c53             1         None\n2  cd0fc7bc53ac632a11643ac4cc91002a             2         None\n2  39a9809abe1441f60f9fab50679c2a73             3         None\n2  396bea28fd4be11598bc53bf5f2b7460             4         None\n2  5d13674e2ed04208bf232e2eeb468f7a             5         None\n3  acccc1727b61b261844d86aa9de53536             1         None\n3  71638b1e853799f227492bfb08a01491             2         None\n3  6770e08049264165b58c5f5ddcfd5b95             3         None\n3  df4b1cfb384dab76abc34cc4db0f5b30             4         None\n3  d4cae2c614676925578ca87836ab95b6             5         None\n4  a3d0e2394d7db36afab1b6e5e24da798             1         None\n4  421afeb5f9dd7b64b84ac8335b14b247             2         None\n4  35293500d91fb681fd8ff9171d8161c1             3         None\n4  33755feadab4a6b5db51edc060585921             4         None\n4  39f3ef0e8046a9b4e6c81262d634053d             5         None\n5  48b243704d16570155df12995a284b61             1         None\n5  fb90604f039d294a931b94442e7f8b73             2         None\n5  2f11b93c073630fc866ebd61708c49c5             3         None\n5  44d35d157f5796ebc04b5fb51c3c1f12             4         None\n5  cb02caed6492a9619e93a3bd265a9c7f             5         None\n6  bd6af21b792a7efb367407e94678fa1f             1         None\n6  5c40def8eb37a29833a498f9390f2b02             2         None\n6  305835a194605ef353a1700def9a2429             3         None\n6  03234e390d3f2c9c07889e1687eafbb8             4         None\n6  e61d36901ebb6b37ac41f6ca84e5c81b             5         None\n7  633ede597d7e514fe672fbdfc8c269eb             1         None\n7  79929caccef83eaa6f23972d59982913             2         None\n7  d5701f000de83388af9171cea1427a7e             3         None\n7  c171fed7a869e9cb41c9a10e7ee6f4f9             4         None\n7  b74f95f32636aff0b94540891c6b1f14             5         None\n8  1b66d3ea1dae116b7c0e87e3caab3340             1         None\n8  8132013f5158a2bc93d77ea9200c75d8             2         None\n8  4285bb1fd50ed9f19bd507e66738189c             3         None\n8  2530e73814b6a250140fab195ac789cc             4         None\n8  271e8bfd46adee65f8580092753d08e5             5         None\n9  7a3df96890c90370590984ca196d1b40             1         None\n9  09c140f91b2da7c7ff024e5f4b2d5599             2         None\n9  5e35ead87bb708f1fea95033ddb2e093             3         None\n9  73d20cbf87ec2637a3de9d950ad809b2             4         None\n9  3c315903764b853c05c059346675e1b7             5         None\n","output_type":"stream"}]},{"cell_type":"code","source":"\nimage_paths=[]\nfor index, row in df.iterrows():\n    image_id = row['image_id']\n    paths = find_images(image_id, image_folder)\n    print(paths)\n    image_paths.append(paths)\n    image_tensor = load_image(paths)\n    print(image_tensor.shape)\n    #df['image_tensor'] = image_tensor\n\n   \n","metadata":{"execution":{"iopub.status.busy":"2024-05-29T13:29:09.768279Z","iopub.execute_input":"2024-05-29T13:29:09.768763Z","iopub.status.idle":"2024-05-29T13:32:16.918574Z","shell.execute_reply.started":"2024-05-29T13:29:09.768728Z","shell.execute_reply":"2024-05-29T13:32:16.917507Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"/kaggle/input/vindr-mammo-dataset/Processed_Images/8269f5971eaca3e5d3772d1796e6bd7a/dd9ce3288c0773e006a294188aadba8e.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/b8d273e8601f348d3664778dae0e7e0b/d8125545210c08e1b1793a5af6458ee2.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/cca141f804a8ab8b584d63102bc7f3f7/aad38467d1f04cedf300c9c8305a3536.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/9ec3039bf215173edf48a8b4f23ad56a/2b46a6fc2c076cfc187b385f6a6f40e6.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/10200894bf6f74b2ae92a0cecd37c60c/581b0e4d471a5c0adc6888cb038fa722.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/8269f5971eaca3e5d3772d1796e6bd7a/57fbdd278af5c8789a02b355c11620d4.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/b8d273e8601f348d3664778dae0e7e0b/290c658f4e75a3f83ec78a847414297c.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/cca141f804a8ab8b584d63102bc7f3f7/2ccefd9e7701acf52c92275e844f7578.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/9ec3039bf215173edf48a8b4f23ad56a/8338207743a394ab8b56b5b920fc30b6.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/10200894bf6f74b2ae92a0cecd37c60c/29896919b15b2dd978a36b2f68a5ad93.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/8269f5971eaca3e5d3772d1796e6bd7a/202d761a6b0f86faaeefc39ee18b1c53.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/b8d273e8601f348d3664778dae0e7e0b/cd0fc7bc53ac632a11643ac4cc91002a.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/7ee98232df9bcefc6b54530b6a616690/39a9809abe1441f60f9fab50679c2a73.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/1734a10c08f2565315493df00a2fd134/396bea28fd4be11598bc53bf5f2b7460.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/450f7f9b2aa32754b076f0f53f0d8efc/5d13674e2ed04208bf232e2eeb468f7a.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/8269f5971eaca3e5d3772d1796e6bd7a/acccc1727b61b261844d86aa9de53536.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/b8d273e8601f348d3664778dae0e7e0b/71638b1e853799f227492bfb08a01491.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/7ee98232df9bcefc6b54530b6a616690/6770e08049264165b58c5f5ddcfd5b95.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/1734a10c08f2565315493df00a2fd134/df4b1cfb384dab76abc34cc4db0f5b30.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/450f7f9b2aa32754b076f0f53f0d8efc/d4cae2c614676925578ca87836ab95b6.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/fa4dcd0f3ba24e86fc8dc25091f7ebd5/a3d0e2394d7db36afab1b6e5e24da798.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/2c314c0a8f364d1a8889bb555d3d04c9/421afeb5f9dd7b64b84ac8335b14b247.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/bdf65210726eb71e919b1af1a1c87c61/35293500d91fb681fd8ff9171d8161c1.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/c800df6e5ea9cd5a183ee765983b4a07/33755feadab4a6b5db51edc060585921.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/fe23c1647f7617ef219a0a0e07c9eec5/39f3ef0e8046a9b4e6c81262d634053d.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/fa4dcd0f3ba24e86fc8dc25091f7ebd5/48b243704d16570155df12995a284b61.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/2c314c0a8f364d1a8889bb555d3d04c9/fb90604f039d294a931b94442e7f8b73.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/bdf65210726eb71e919b1af1a1c87c61/2f11b93c073630fc866ebd61708c49c5.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/c800df6e5ea9cd5a183ee765983b4a07/44d35d157f5796ebc04b5fb51c3c1f12.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/fe23c1647f7617ef219a0a0e07c9eec5/cb02caed6492a9619e93a3bd265a9c7f.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/fa4dcd0f3ba24e86fc8dc25091f7ebd5/bd6af21b792a7efb367407e94678fa1f.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/7ee98232df9bcefc6b54530b6a616690/5c40def8eb37a29833a498f9390f2b02.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/72f7758250782ee835586f7122acd9fd/305835a194605ef353a1700def9a2429.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/72d177eae7c6b34a411341368f32ca5a/03234e390d3f2c9c07889e1687eafbb8.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/a0da8d047c71765cdf217a8a52606741/e61d36901ebb6b37ac41f6ca84e5c81b.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/fa4dcd0f3ba24e86fc8dc25091f7ebd5/633ede597d7e514fe672fbdfc8c269eb.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/7ee98232df9bcefc6b54530b6a616690/79929caccef83eaa6f23972d59982913.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/72f7758250782ee835586f7122acd9fd/d5701f000de83388af9171cea1427a7e.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/72d177eae7c6b34a411341368f32ca5a/c171fed7a869e9cb41c9a10e7ee6f4f9.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/a0da8d047c71765cdf217a8a52606741/b74f95f32636aff0b94540891c6b1f14.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/0a0c5108270e814818c1ad002482ce74/1b66d3ea1dae116b7c0e87e3caab3340.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/522963c771a84cb777c49ba7a4ca69fc/8132013f5158a2bc93d77ea9200c75d8.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/34317eec7ecb8de8f9c71bb55abb5ce4/4285bb1fd50ed9f19bd507e66738189c.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/d010dbb93a1a164ce8afc0b62d22de98/2530e73814b6a250140fab195ac789cc.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/a0d75cb5a7be01a0d63f5d9ec553443e/271e8bfd46adee65f8580092753d08e5.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/0a0c5108270e814818c1ad002482ce74/7a3df96890c90370590984ca196d1b40.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/522963c771a84cb777c49ba7a4ca69fc/09c140f91b2da7c7ff024e5f4b2d5599.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/34317eec7ecb8de8f9c71bb55abb5ce4/5e35ead87bb708f1fea95033ddb2e093.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/d010dbb93a1a164ce8afc0b62d22de98/73d20cbf87ec2637a3de9d950ad809b2.png\ntorch.Size([3, 224, 224])\n/kaggle/input/vindr-mammo-dataset/Processed_Images/a0d75cb5a7be01a0d63f5d9ec553443e/3c315903764b853c05c059346675e1b7.png\ntorch.Size([3, 224, 224])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"print(image_paths)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T09:54:23.426903Z","iopub.execute_input":"2024-05-29T09:54:23.427898Z","iopub.status.idle":"2024-05-29T09:54:23.435368Z","shell.execute_reply.started":"2024-05-29T09:54:23.427856Z","shell.execute_reply":"2024-05-29T09:54:23.433633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install vit-pytorch\n","metadata":{"execution":{"iopub.status.busy":"2024-05-29T13:32:47.618918Z","iopub.execute_input":"2024-05-29T13:32:47.619350Z","iopub.status.idle":"2024-05-29T13:33:03.029408Z","shell.execute_reply.started":"2024-05-29T13:32:47.619309Z","shell.execute_reply":"2024-05-29T13:33:03.027969Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Requirement already satisfied: vit-pytorch in /opt/conda/lib/python3.10/site-packages (1.6.9)\nRequirement already satisfied: einops>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from vit-pytorch) (0.8.0)\nRequirement already satisfied: torch>=1.10 in /opt/conda/lib/python3.10/site-packages (from vit-pytorch) (2.1.2+cpu)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from vit-pytorch) (0.16.2+cpu)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10->vit-pytorch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10->vit-pytorch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10->vit-pytorch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10->vit-pytorch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10->vit-pytorch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10->vit-pytorch) (2024.2.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->vit-pytorch) (1.26.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision->vit-pytorch) (2.31.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->vit-pytorch) (9.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10->vit-pytorch) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->vit-pytorch) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->vit-pytorch) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->vit-pytorch) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->vit-pytorch) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10->vit-pytorch) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install timm","metadata":{"execution":{"iopub.status.busy":"2024-05-29T13:32:32.177999Z","iopub.execute_input":"2024-05-29T13:32:32.178425Z","iopub.status.idle":"2024-05-29T13:32:47.613580Z","shell.execute_reply.started":"2024-05-29T13:32:32.178383Z","shell.execute_reply":"2024-05-29T13:32:47.611773Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Requirement already satisfied: timm in /opt/conda/lib/python3.10/site-packages (0.9.16)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from timm) (2.1.2+cpu)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from timm) (0.16.2+cpu)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from timm) (6.0.1)\nRequirement already satisfied: huggingface_hub in /opt/conda/lib/python3.10/site-packages (from timm) (0.22.2)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm) (0.4.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (2024.2.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (21.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (4.66.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub->timm) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->timm) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->timm) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->timm) (3.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->timm) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->timm) (9.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub->timm) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->timm) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface_hub->timm) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->timm) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torchvision.models import resnet50, resnet152, densenet121\nfrom vit_pytorch import ViT\nimport timm\nimport torch\n\ndef load_model(model_type, checkpoint_path):\n    if model_type == 'resnet50':\n        model = resnet50(pretrained=True)\n        num_ftrs = model.fc.in_features\n        model.fc = torch.nn.Linear(num_ftrs, 512)\n    elif model_type == 'resnet152':\n        model = resnet152(pretrained=False)\n    elif model_type == 'densenet121':\n        model = densenet121(pretrained=True)\n        num_classes = 512\n        model.classifier = torch.nn.Linear(model.classifier.in_features, num_classes)\n    elif model_type == 'vit':\n        model_name = 'vit_base_patch16_224'  # Specify the model name\n        num_classes = 512  # Number of output classes (adjust as needed)\n        model = timm.create_model(model_name, pretrained=False, num_classes=num_classes)\n\n    else:\n        raise ValueError(f\"Loại mô hình '{model_type}' không được hỗ trợ.\")\n    \n    # Load checkpoint\n    checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n    \n    # Extract the model state dictionary from the checkpoint\n    if 'model_state_dict' in checkpoint:\n        state_dict = checkpoint['model_state_dict']\n    else:\n        state_dict = checkpoint  # Assuming the entire checkpoint is the state_dict\n    \n    # Adjust state dictionary keys if necessary\n    new_state_dict = {k.replace('model.', ''): v for k, v in state_dict.items()}\n    model.load_state_dict(new_state_dict)\n    return model\n    \n    # Load the state\n","metadata":{"execution":{"iopub.status.busy":"2024-05-29T13:34:53.237496Z","iopub.execute_input":"2024-05-29T13:34:53.238053Z","iopub.status.idle":"2024-05-29T13:34:54.974018Z","shell.execute_reply.started":"2024-05-29T13:34:53.237996Z","shell.execute_reply":"2024-05-29T13:34:54.972793Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"checkpoint='/kaggle/input/ver1-checkpoint/version 1/weights_setting2/densenet121BasedModel/best.pt'\nmodel=load_model('densenet121',checkpoint)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-29T14:28:55.806250Z","iopub.execute_input":"2024-05-29T14:28:55.806780Z","iopub.status.idle":"2024-05-29T14:28:56.230686Z","shell.execute_reply.started":"2024-05-29T14:28:55.806743Z","shell.execute_reply":"2024-05-29T14:28:56.229396Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"def rank_columns(matrix):\n    num_rows, num_cols = matrix.shape\n    rank_matrix = torch.zeros(matrix.shape, dtype=torch.int32)\n\n    for col in range(num_cols):\n        # Get the values in the column\n        column_values = matrix[:, col]\n        # Sort the values and get the indices of the sorted values\n        sorted_indices = torch.argsort(column_values, descending=True)\n        # Assign ranks based on the sorted indices\n        for rank, idx in enumerate(sorted_indices):\n            rank_matrix[idx, col] = rank + 1  # Rank starts from 1\n\n    return rank_matrix\n\n\n# Get the rank matrix\nmatrix=[[0,-2,3],[2,0,1],[3,4,0]]\nsmatrix = torch.tensor(matrix, dtype=torch.float32)\nrank_matrix = rank_columns(smatrix)\nprint(rank_matrix)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T14:06:34.716969Z","iopub.execute_input":"2024-05-29T14:06:34.717438Z","iopub.status.idle":"2024-05-29T14:06:34.729942Z","shell.execute_reply.started":"2024-05-29T14:06:34.717405Z","shell.execute_reply":"2024-05-29T14:06:34.728509Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"tensor([[3, 3, 1],\n        [2, 2, 2],\n        [1, 1, 3]], dtype=torch.int32)\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport torch.nn.functional as F\n\ndef save_matrix(df,model):\n    n = 5\n    matrix_root = np.zeros((n, n)) \n    matrix_root_tensor = torch.tensor(matrix_root)\n    for i in range(0, 10):\n        output = []\n        matrix = np.zeros((n, n))  \n        selected_rows = result.loc[result.index == i]\n        for index, row in selected_rows.iterrows():\n            image_id = row['image_id']\n            paths = find_images(image_id, image_folder)\n            print(image_id,row['breast_birads'])\n            input_tensor = load_image(paths)\n            input_batch = input_tensor.unsqueeze(0)\n\n            if torch.cuda.is_available():\n                input_batch = input_batch.to('cuda')\n                model.to('cuda')\n\n            with torch.no_grad():\n                out = model(input_batch)\n                output.append(out)\n        for i in range(0, 5):\n            for j in range(0, 5):\n                if i == j:\n                    matrix[i, j]=1\n                else:\n                    matrix[i, j] = F.cosine_similarity(output[i], output[j], dim=1)\n                    matrix[j, i] = matrix[i, j]  # Since it's symmetric, fill the corresponding element\n        matrix = torch.tensor(matrix, dtype=torch.float32)\n        print(matrix)\n        rank_matrix = rank_columns(matrix)\n        print(rank_matrix)\n        matrix_root_tensor=matrix_root_tensor+rank_matrix\n        \n        \n        \n    return matrix_root_tensor\n","metadata":{"execution":{"iopub.status.busy":"2024-05-29T14:06:37.889859Z","iopub.execute_input":"2024-05-29T14:06:37.890298Z","iopub.status.idle":"2024-05-29T14:06:37.906400Z","shell.execute_reply.started":"2024-05-29T14:06:37.890264Z","shell.execute_reply":"2024-05-29T14:06:37.904800Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"matrix=save_matrix(df,model)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T14:29:02.731735Z","iopub.execute_input":"2024-05-29T14:29:02.732284Z","iopub.status.idle":"2024-05-29T14:32:27.632836Z","shell.execute_reply.started":"2024-05-29T14:29:02.732246Z","shell.execute_reply":"2024-05-29T14:32:27.631380Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"dd9ce3288c0773e006a294188aadba8e 1\nd8125545210c08e1b1793a5af6458ee2 2\naad38467d1f04cedf300c9c8305a3536 3\n2b46a6fc2c076cfc187b385f6a6f40e6 4\n581b0e4d471a5c0adc6888cb038fa722 5\ntensor([[1.0000, 0.7642, 0.6576, 0.7100, 0.7570],\n        [0.7642, 1.0000, 0.6920, 0.6916, 0.7227],\n        [0.6576, 0.6920, 1.0000, 0.6266, 0.6231],\n        [0.7100, 0.6916, 0.6266, 1.0000, 0.7027],\n        [0.7570, 0.7227, 0.6231, 0.7027, 1.0000]])\ntensor([[1, 2, 3, 2, 2],\n        [2, 1, 2, 4, 3],\n        [5, 4, 1, 5, 5],\n        [4, 5, 4, 1, 4],\n        [3, 3, 5, 3, 1]], dtype=torch.int32)\n57fbdd278af5c8789a02b355c11620d4 1\n290c658f4e75a3f83ec78a847414297c 2\n2ccefd9e7701acf52c92275e844f7578 3\n8338207743a394ab8b56b5b920fc30b6 4\n29896919b15b2dd978a36b2f68a5ad93 5\ntensor([[1.0000, 0.7253, 0.7053, 0.6691, 0.6655],\n        [0.7253, 1.0000, 0.6918, 0.6797, 0.6565],\n        [0.7053, 0.6918, 1.0000, 0.6774, 0.6580],\n        [0.6691, 0.6797, 0.6774, 1.0000, 0.6367],\n        [0.6655, 0.6565, 0.6580, 0.6367, 1.0000]])\ntensor([[1, 2, 2, 4, 2],\n        [2, 1, 3, 2, 4],\n        [3, 3, 1, 3, 3],\n        [4, 4, 4, 1, 5],\n        [5, 5, 5, 5, 1]], dtype=torch.int32)\n202d761a6b0f86faaeefc39ee18b1c53 1\ncd0fc7bc53ac632a11643ac4cc91002a 2\n39a9809abe1441f60f9fab50679c2a73 3\n396bea28fd4be11598bc53bf5f2b7460 4\n5d13674e2ed04208bf232e2eeb468f7a 5\ntensor([[1.0000, 0.7605, 0.7601, 0.6979, 0.7864],\n        [0.7605, 1.0000, 0.7598, 0.6704, 0.7719],\n        [0.7601, 0.7598, 1.0000, 0.7082, 0.7685],\n        [0.6979, 0.6704, 0.7082, 1.0000, 0.6433],\n        [0.7864, 0.7719, 0.7685, 0.6433, 1.0000]])\ntensor([[1, 3, 3, 3, 2],\n        [3, 1, 4, 4, 3],\n        [4, 4, 1, 2, 4],\n        [5, 5, 5, 1, 5],\n        [2, 2, 2, 5, 1]], dtype=torch.int32)\nacccc1727b61b261844d86aa9de53536 1\n71638b1e853799f227492bfb08a01491 2\n6770e08049264165b58c5f5ddcfd5b95 3\ndf4b1cfb384dab76abc34cc4db0f5b30 4\nd4cae2c614676925578ca87836ab95b6 5\ntensor([[1.0000, 0.6773, 0.6166, 0.6204, 0.6823],\n        [0.6773, 1.0000, 0.6684, 0.6578, 0.7237],\n        [0.6166, 0.6684, 1.0000, 0.7122, 0.6739],\n        [0.6204, 0.6578, 0.7122, 1.0000, 0.7094],\n        [0.6823, 0.7237, 0.6739, 0.7094, 1.0000]])\ntensor([[1, 3, 5, 5, 4],\n        [3, 1, 4, 4, 2],\n        [5, 4, 1, 2, 5],\n        [4, 5, 2, 1, 3],\n        [2, 2, 3, 3, 1]], dtype=torch.int32)\na3d0e2394d7db36afab1b6e5e24da798 1\n421afeb5f9dd7b64b84ac8335b14b247 2\n35293500d91fb681fd8ff9171d8161c1 3\n33755feadab4a6b5db51edc060585921 4\n39f3ef0e8046a9b4e6c81262d634053d 5\ntensor([[1.0000, 0.7432, 0.7744, 0.6946, 0.7754],\n        [0.7432, 1.0000, 0.7586, 0.6882, 0.7433],\n        [0.7744, 0.7586, 1.0000, 0.7132, 0.7668],\n        [0.6946, 0.6882, 0.7132, 1.0000, 0.7101],\n        [0.7754, 0.7433, 0.7668, 0.7101, 1.0000]])\ntensor([[1, 4, 2, 4, 2],\n        [4, 1, 4, 5, 4],\n        [3, 2, 1, 2, 3],\n        [5, 5, 5, 1, 5],\n        [2, 3, 3, 3, 1]], dtype=torch.int32)\n48b243704d16570155df12995a284b61 1\nfb90604f039d294a931b94442e7f8b73 2\n2f11b93c073630fc866ebd61708c49c5 3\n44d35d157f5796ebc04b5fb51c3c1f12 4\ncb02caed6492a9619e93a3bd265a9c7f 5\ntensor([[1.0000, 0.6333, 0.7142, 0.6839, 0.7671],\n        [0.6333, 1.0000, 0.6478, 0.6332, 0.7141],\n        [0.7142, 0.6478, 1.0000, 0.6729, 0.7453],\n        [0.6839, 0.6332, 0.6729, 1.0000, 0.7264],\n        [0.7671, 0.7141, 0.7453, 0.7264, 1.0000]])\ntensor([[1, 4, 3, 3, 2],\n        [5, 1, 5, 5, 5],\n        [3, 3, 1, 4, 3],\n        [4, 5, 4, 1, 4],\n        [2, 2, 2, 2, 1]], dtype=torch.int32)\nbd6af21b792a7efb367407e94678fa1f 1\n5c40def8eb37a29833a498f9390f2b02 2\n305835a194605ef353a1700def9a2429 3\n03234e390d3f2c9c07889e1687eafbb8 4\ne61d36901ebb6b37ac41f6ca84e5c81b 5\ntensor([[1.0000, 0.6766, 0.6845, 0.6780, 0.6850],\n        [0.6766, 1.0000, 0.7112, 0.6948, 0.6927],\n        [0.6845, 0.7112, 1.0000, 0.7380, 0.7466],\n        [0.6780, 0.6948, 0.7380, 1.0000, 0.7290],\n        [0.6850, 0.6927, 0.7466, 0.7290, 1.0000]])\ntensor([[1, 5, 5, 5, 5],\n        [5, 1, 4, 4, 4],\n        [3, 2, 1, 2, 2],\n        [4, 3, 3, 1, 3],\n        [2, 4, 2, 3, 1]], dtype=torch.int32)\n633ede597d7e514fe672fbdfc8c269eb 1\n79929caccef83eaa6f23972d59982913 2\nd5701f000de83388af9171cea1427a7e 3\nc171fed7a869e9cb41c9a10e7ee6f4f9 4\nb74f95f32636aff0b94540891c6b1f14 5\ntensor([[1.0000, 0.6068, 0.6875, 0.6441, 0.6513],\n        [0.6068, 1.0000, 0.6616, 0.6200, 0.6858],\n        [0.6875, 0.6616, 1.0000, 0.6562, 0.7039],\n        [0.6441, 0.6200, 0.6562, 1.0000, 0.6696],\n        [0.6513, 0.6858, 0.7039, 0.6696, 1.0000]])\ntensor([[1, 5, 3, 4, 5],\n        [5, 1, 4, 5, 3],\n        [2, 3, 1, 3, 2],\n        [4, 4, 5, 1, 4],\n        [3, 2, 2, 2, 1]], dtype=torch.int32)\n1b66d3ea1dae116b7c0e87e3caab3340 1\n8132013f5158a2bc93d77ea9200c75d8 2\n4285bb1fd50ed9f19bd507e66738189c 3\n2530e73814b6a250140fab195ac789cc 4\n271e8bfd46adee65f8580092753d08e5 5\ntensor([[1.0000, 0.7327, 0.6945, 0.7281, 0.6789],\n        [0.7327, 1.0000, 0.7110, 0.7428, 0.7133],\n        [0.6945, 0.7110, 1.0000, 0.6950, 0.6800],\n        [0.7281, 0.7428, 0.6950, 1.0000, 0.7241],\n        [0.6789, 0.7133, 0.6800, 0.7241, 1.0000]])\ntensor([[1, 3, 4, 3, 5],\n        [2, 1, 2, 2, 3],\n        [4, 5, 1, 5, 4],\n        [3, 2, 3, 1, 2],\n        [5, 4, 5, 4, 1]], dtype=torch.int32)\n7a3df96890c90370590984ca196d1b40 1\n09c140f91b2da7c7ff024e5f4b2d5599 2\n5e35ead87bb708f1fea95033ddb2e093 3\n73d20cbf87ec2637a3de9d950ad809b2 4\n3c315903764b853c05c059346675e1b7 5\ntensor([[1.0000, 0.6108, 0.6706, 0.6837, 0.6368],\n        [0.6108, 1.0000, 0.6690, 0.6853, 0.6804],\n        [0.6706, 0.6690, 1.0000, 0.7583, 0.6977],\n        [0.6837, 0.6853, 0.7583, 1.0000, 0.6972],\n        [0.6368, 0.6804, 0.6977, 0.6972, 1.0000]])\ntensor([[1, 5, 4, 5, 5],\n        [5, 1, 5, 4, 4],\n        [3, 4, 1, 2, 2],\n        [2, 2, 2, 1, 3],\n        [4, 3, 3, 3, 1]], dtype=torch.int32)\n","output_type":"stream"}]},{"cell_type":"code","source":"print(matrix/10)\n#weights_setting2/densenetBasedModel/best.pt\n","metadata":{"execution":{"iopub.status.busy":"2024-05-29T14:33:18.188098Z","iopub.execute_input":"2024-05-29T14:33:18.188645Z","iopub.status.idle":"2024-05-29T14:33:18.197961Z","shell.execute_reply.started":"2024-05-29T14:33:18.188591Z","shell.execute_reply":"2024-05-29T14:33:18.195968Z"},"trusted":true},"execution_count":71,"outputs":[{"name":"stdout","text":"tensor([[1.0000, 3.6000, 3.4000, 3.8000, 3.4000],\n        [3.6000, 1.0000, 3.7000, 3.9000, 3.5000],\n        [3.5000, 3.4000, 1.0000, 3.0000, 3.3000],\n        [3.9000, 4.0000, 3.7000, 1.0000, 3.8000],\n        [3.0000, 3.0000, 3.2000, 3.3000, 1.0000]], dtype=torch.float64)\n","output_type":"stream"}]}]}